# 广告后台服务化方案

## 背景
   商业平台业务日趋复杂，随之代码仓库日趋庞大，代码几乎全部集中在adweb和ss_ad两个仓库，随着业务的快速发展，后期代码膨胀将难以维护.
   
## 缺点
   * 不同业务全部集中在一个系统里，代码的开发、线上维护互相影响
   * 代码接口、方法通过引入模块进行依赖，模块间无法做到完全透明，不够轻量
   * 业务职能边界不够清晰，不收敛，很难做到单一职责
   
## 服务化方案
   * 系统按照不同的业务进行业务模块划分，职责收敛
   * 不同业务模块对外提供rpc服务，不再进行代码依赖，而是服务接口依赖
   * 将controller层单独抽离成一个系统模块，该模块只负责将http请求转发给后端rpc服务，以及多种rpc服务的组合，业务逻辑全部下沉到各个rpc服务

## 服务化模块
   根据现有商业平台业务功能进行划分，将分为以下八大中心，两大web系统
   
1、八大中心

   * 推广中心：负责adgroup，campaign，creative的管理
   * 账户中心：负责广告主，代理商的操作权限，登录的管理
   * 日志中心：负责广告主操作日志管理
   * 工具中心：负责平台工具管理
   * 报表中心：负责平台报表数据管理
   * 消息中心：负责平台站内信管理
   * 库存中心：负责库存管理
   * 财务中心：负责平台财务结算管理
   
2、两大web系统

   * 商业平台前端系统：负责http请求处理，业务数据响应，rpc服务调用，前端模板渲染
   * open api网关系统：通过配置化方式将rpc服务能力对外以http接口透出，负责open api的参数到rpc服务参数的转化，参数合法性校验，rpc服务编排
   
## 服务化架构图
![](file:///Users/liboyang/github/newgit1/lALPAAAAARPYcuTM7s0B1A_468_238.png)

## 迁移方案
   因为业务迭代的速度很快，而将现有复杂系统进行服务化拆分是一个非常耗时漫长的过程，因此迁移步骤尤为重要，步骤如下：
   
### 代码迁移
   
     1、先将业务最稳定模块服务化拆出，目前商业平台广告推广主流程最为稳定，如创建adgroup、campaign
     2、创建以runtime框架为基础的rpc服务工程，并规范好工程的代码规范，目录结构
     3、将adgroup和campaign的业务逻辑迁移到该工程（推广中心），提供rpc服务接口
     4、因为是新工程，尽量采用新平台支撑: 
        (1) gerrit迁移gitlab
        (2) 接入tce
        
### 流量迁移

   推广中心上线后，会接入线上adgroup和campaign的业务流量，即使线下测试无问题，也不能完全保证推广中心代码逻辑正确性，因此需要引入线上流量进行一段时间观察对比和验证，后面方案的流量切分将会应用小流量平台化方案进行实施
   目前有三个方案：
   
##### 方案一
   
###### 说明
    1、全量copy一份线上白名单客户的adgroup和campaign数据表，称为影子表
    2、在ad_web上引入双读双写middleware，拦截所有请求，但只对白名单客户进行流量的双读双写
    3、对客户端的读写数据响应此时都以原接口为准
    4、middleware将双读结果发送到kafka,由jstorm异步消费，把不一致数据写日志落盘，并同步es
    4、双写结果则由原表与影子表数据对比，每一小时离线对比一次
    6、对比一段时间无问题后，关闭镜像流量模式,将新接口的影子表切换到老表后，开启流量切分模式，对客户端的读写数据切换到以新接口为准
    7、存在非幂等的下游消息消费端，当在镜像流量模式下，所有消息的发送和消费都需要mock；在流量切分模式下，所有消息的发送和消费都需要开启
    8、以上步骤都需要增加开关控制，能做到如果新接口有问题，可以迅速切回老接口
    
    
    缺陷：线上切流过程周期长，步骤复杂
    优点：能保证读写数据的强一致性，能做到迁移方案的通用性，可以做成统一迁移方案平台
    
##### 迁移框图
   
   ![](file:///Users/liboyang/github/newgit1/lALPAAAAARSefFjNAaHNAnY_630_417.png) 
   

##### 方案二
    1、在ad_web引入根据用户白名单进行流量切分的middleware，与方案一不同的是流量不在镜像一份，切流过程流量从小到大逐步验证
    2、在白名单的用户流量进入推广中心，读写都已新接口为准
    3、与老接口一样写线上库，只是白名单用户数据由新接口写入
    
    缺陷：对于被白名单而切流的客户读写数据不能及时进行对比，需要用户反馈，并人工确认，一旦数据有问题，需要人工订正，并不断修复上线，需要测试环节给力，
    优点：能够快速迭代，周期短，不要求下游有很强的幂等
    
##### 迁移框图
   
   ![](file:///Users/liboyang/github/newgit1/lALPAAAAARSt403M-c0BZw_359_249.png) 
   
   
##### 方案三
    1、在线上adweb引入流量采集middleware，根据白名单采集线上流量，采集包括请求，入参，返回结果
    2、将线上采集的流量通过异步消息队列在线下方案二架构图中心进行流量回放
    3、将线下回放结果与线上流量结果对比，这里只对比到db前的sql
    4、对比一段时间无问题后，上线观察，此时切换到方案二中的流量切分模式，切流过程由小到大
    
    缺陷：需要对数据库连接层进行代码封装，有一定的代码侵入性
    优点：能够用真实流量进行线下充分测试，保证线上切流过程顺利进行，同时线上、线下环境隔离，无相干扰
    
##### 迁移框图
   
   ![](file:///Users/liboyang/github/newgit1/lALPAAAAARSt403M-c0BZw_359_249.png) 

## 后续
当各个模块业务能够单一职责，所有业务代码变动只会影响相关模块，其它模块只关心接口服务；同时，开发同学可以更加深钻自己负责的领域模型和业务，成为领域专家。
对于日常开发，提供所有模块都能正常运行的稳定测试环境，而如果某次需求只涉及创意，只需要clone推广中心代码，修改前端rpc服务为本地服务版本号即可进行测试、联调，极大提升开发效率。
对于新人，可以跳过繁琐的搭建开发环境的过程，快速学习，快速投入开发工作。